{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "850071f1",
   "metadata": {},
   "source": [
    "# Adversarial robustness demos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b4c58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import robustbench as rb\n",
    "import art\n",
    "import foolbox as fb\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import gc\n",
    "\n",
    "from autoattack import AutoAttack\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fddb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '.'\n",
    "MODEL_DIR = f'{PATH}/models'\n",
    "DATA_DIR = f'{PATH}/data'\n",
    "CKPT_PATH = f'{PATH}/checkpoint'\n",
    "DEVICE = 'cuda:0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d9ff1c",
   "metadata": {},
   "source": [
    "## The CIFAR-10 data set\n",
    "\n",
    "The [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) data set was originally derived in 2009 from the [80 Million Tiny Images](https://groups.csail.mit.edu/vision/TinyImages/) data set. Despite being over 20 years old, it remains one of the most heavily-used data sets for benchmarking novel machine learning algorithms (especially robustness). It consists of 70,000 RGB images 32x32 pixels in size, divided into ten classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship and truck. Samples from CIFAR-10 can be obtained easily via RobustBench:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d77061",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data, y_data = rb.data.load_cifar10(n_examples=10, data_dir=DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf2198f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_name(label):\n",
    "    names = [\n",
    "        'airplane',\n",
    "        'automobile',\n",
    "        'bird',\n",
    "        'cat',\n",
    "        'deer',\n",
    "        'dog',\n",
    "        'frog',\n",
    "        'horse',\n",
    "        'ship',\n",
    "        'truck'\n",
    "    ]\n",
    "    return names[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a4e66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 5)\n",
    "for i in range(2):\n",
    "    for j in range(5):\n",
    "        k = 5*i + j\n",
    "        axes[i, j].set_title(to_name(y_data[k]))\n",
    "        axes[i, j].imshow(np.transpose(x_data[k], [1, 2, 0]))\n",
    "        axes[i, j].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c5d147",
   "metadata": {},
   "source": [
    "The full CIFAR-10 data set is also available via [torchvision](https://pytorch.org/vision/0.9/datasets.html#cifar) or the [TensorFlow data sets catalog](https://www.tensorflow.org/datasets). An extension known as CIFAR-100 is sometimes also used for benchmarking robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4773a87",
   "metadata": {},
   "source": [
    "## The fast gradient sign method\n",
    "\n",
    "As a first demonstration of how incredibly sensitive state of the art deep learning models can be, let us load a SOTA model from the [RobustBench model zoo](https://github.com/RobustBench/robustbench#model-zoo-quick-tour):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e17fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_model = rb.load_model(model_name='Standard', dataset='cifar10', model_dir=MODEL_DIR).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2f23f8",
   "metadata": {},
   "source": [
    "The model in question is a WideResNet architecture trained via standard gradient descent methods that performs very well on CIFAR-10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fff94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test = rb.data.load_cifar10(n_examples=128, data_dir=DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2868abe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = rb.utils.clean_accuracy(standard_model, x_test, y_test, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c40aaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Standard accuracy: {acc:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7f64de",
   "metadata": {},
   "source": [
    "The FGS method approximates the model loss using a first-order Taylor expansion:\n",
    "$$\n",
    "    L(f, x + \\delta, y) \\approx L(f, x, y) + \\langle \\delta, \\nabla_x L(f, x, y) \\rangle.\n",
    "$$\n",
    "Hence, to maximize the loss of $f$ at $x + \\delta$, we need only maximize the inner product between the perturbation vector $\\delta$ and the gradient vector of the loss, $\\nabla_x L(f, x, y)$, provided $\\delta$ remains small. This is easy to do by setting\n",
    "$$\n",
    "    \\delta = \\varepsilon \\cdot \\mathrm{sgn}(\\nabla_x L(f, x, y))\n",
    "$$\n",
    "for some small $\\varepsilon > 0$. In that case,\n",
    "$$\\begin{aligned}\n",
    "    L(f, x + \\delta, y)\n",
    "    &\\approx L(f, x, y) + \\langle \\delta, \\nabla_x L(f, x, y) \\rangle\\\\\n",
    "    &= L(f, x, y) + \\varepsilon\\|\\nabla_x L(f, x, y)\\|_1.\n",
    "\\end{aligned}$$\n",
    "Note that $\\|\\nabla_x L(f, x, y)\\|_1$ is roughly proportional to the dimensionality of the input:\n",
    "$$\n",
    "    \\|\\nabla_x L(f, x, y)\\|_1 = \\sum_{i=1}^n\\left|\\frac{\\partial}{\\partial x_i}L(f, x, y)\\right| = \\mathcal{O}(n).\n",
    "$$\n",
    "Therefore, even if $\\varepsilon$ (and hence $\\delta$) is small, the increase in loss can be very large if the data is high dimensional (which it often is).\n",
    "\n",
    "Let's put this reasoning to the test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615063a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = x_test.clone().detach().requires_grad_().to(DEVICE)\n",
    "labels = y_test.to(DEVICE)\n",
    "\n",
    "samples.retain_grad()\n",
    "\n",
    "loss = F.nll_loss(standard_model(samples), labels)\n",
    "loss.backward()\n",
    "\n",
    "x_grad = torch.sign(samples.grad)\n",
    "\n",
    "epsilons = np.linspace(0, .03, 100)\n",
    "accs = []\n",
    "with torch.no_grad():\n",
    "    for eps in tqdm(epsilons):\n",
    "        delta = eps * x_grad\n",
    "        x_tilde = torch.clip(samples + delta, 0, 1)\n",
    "\n",
    "        acc = rb.utils.clean_accuracy(standard_model, x_tilde, labels)\n",
    "        accs.append(acc)\n",
    "        \n",
    "        del x_tilde\n",
    "\n",
    "del samples\n",
    "del labels\n",
    "del x_grad\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829c2cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epsilons, accs)\n",
    "plt.xlabel('epsilon')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72f3d3f",
   "metadata": {},
   "source": [
    "We see here that the accuracy of this model declines rapidly with increasing values of $\\varepsilon$: at $\\varepsilon \\approx 0.03$, the model accuracy has already dropped from over 94% to less than 40%!\n",
    "\n",
    "It is interesting to visualize these perturbations, to get a sense of just how insignificant they are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f69a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = .03\n",
    "\n",
    "samples = x_test.clone().detach().requires_grad_().to(DEVICE)\n",
    "labels = y_test.to(DEVICE)\n",
    "\n",
    "samples.retain_grad()\n",
    "\n",
    "loss = F.nll_loss(standard_model(samples), labels)\n",
    "loss.backward()\n",
    "\n",
    "with torch.no_grad():\n",
    "    x_tilde = torch.clip(samples + epsilon * torch.sign(samples.grad), 0, 1)\n",
    "    y_tilde = standard_model(x_tilde).cpu().detach().numpy()\n",
    "\n",
    "    x_tilde = x_tilde.cpu().detach().numpy()\n",
    "    \n",
    "    y_pred = standard_model(samples).cpu().detach().numpy()\n",
    "\n",
    "num_examples = 10\n",
    "for i in range(num_examples):\n",
    "    idx = np.random.randint(0, x_test.shape[0])\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2)\n",
    "    \n",
    "    axes[0].axis('off')\n",
    "    axes[0].set_title(f'original: {to_name(y_pred[idx].argmax())}')\n",
    "    axes[0].imshow(np.transpose(x_test[idx], [1, 2, 0]))\n",
    "    \n",
    "    axes[1].axis('off')\n",
    "    axes[1].set_title(f'perturbed: {to_name(y_tilde[idx].argmax())}')\n",
    "    axes[1].imshow(np.clip(np.transpose(x_tilde[idx], [1, 2, 0]), 0, 1))\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "del samples\n",
    "del labels\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea241be",
   "metadata": {},
   "source": [
    "The perturbations are visible in the image, of course, but they have essentially no effect on what is depicted. The fact that state of the art, highly accurate models could be so vulnerable to such seemingly insignificant perturbations came as a major shock to the machine learning community back in 2013 when this phenomenon was first described ([Szegedy et al. (2013)](https://arxiv.org/pdf/1312.6199.pdf)). The algorithm we implemented here is known as the **fast gradient sign method** (FGSM). Proposed by [Goodfellow et al.](https://arxiv.org/pdf/1412.6572.pdf) in 2014, it was the first *efficient* algorithm for generating adversarial examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56efefe",
   "metadata": {},
   "source": [
    "## The adversarial robustness toolbox\n",
    "\n",
    "Of course, by now there exist many open source libraries that implement a variety of attacks and defenses for us, so we don't have to program all of this by hand. Here's an example where I use the well-known [Adversarial Robustness Toolbox (ART)](https://adversarial-robustness-toolbox.readthedocs.io/en/latest/) by IBM to generate adversarial examples against the vanilla model using the basic iterative method (BIM):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e443257c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from art.attacks.evasion import BasicIterativeMethod\n",
    "from art.estimators.classification import PyTorchClassifier\n",
    "\n",
    "attack = BasicIterativeMethod(\n",
    "    PyTorchClassifier(\n",
    "        model=standard_model,\n",
    "        clip_values=(0, 1),\n",
    "        loss=torch.nn.CrossEntropyLoss(),\n",
    "        input_shape=[3, 32, 32],\n",
    "        nb_classes=10),\n",
    "    eps=.03,\n",
    "    eps_step=.001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec337e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_advs = attack.generate(x=x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b339c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = rb.utils.clean_accuracy(standard_model, torch.from_numpy(x_advs), y_test, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688e7ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Accuracy: {acc:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3503fb8",
   "metadata": {},
   "source": [
    "Where our own implementation of FGSM managed to lower the model accuracy to just under 40% at $\\varepsilon = .03$, the BIM reduces it to less than 10% (the level of random guessing on the CIFAR-10 data set). This is to be expected: BIM is a simple iterative version of FGS, where we essentially just perform several iterations of FGS, projecting the result back onto the appropriate $L_\\infty$ ball each time. In pseudo-code, we can describe it as\n",
    "\n",
    "    x[0] = x_orig\n",
    "    for t = 1 to T:\n",
    "        x[t] = FGSM(x[t-1])\n",
    "    return x[T]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7ad6d2",
   "metadata": {},
   "source": [
    "## The AutoAttack benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac8e3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "adversary = AutoAttack(standard_model, norm='Linf', eps=.03, version='standard')\n",
    "\n",
    "images = x_test.to(DEVICE)\n",
    "labels = y_test.to(DEVICE)\n",
    "\n",
    "x_adv = adversary.run_standard_evaluation(images, labels, bs=128)\n",
    "\n",
    "del images\n",
    "del labels\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0e8e78",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec68f04a",
   "metadata": {},
   "source": [
    "## Adversarial training\n",
    "\n",
    "A straightforward idea for defense is **data augmentation**: simply expand the data set to include adversarial examples, so the model will learn to be robust to them the same way it learns everything else. An important design decision to implement this strategy is the specific choice of adversarial attack(s) on which to train. In particular, if the attack is *white-box* (as the strongest attacks often are), the generated adversarials will depend on the model parameters. Even if the attack is not white-box and does not need access to the model parameters, it might still be randomized and hence give different results for different runs on the same data.\n",
    "\n",
    "These properties of adversarial attacks force us to adopt a slightly more complicated scheme, as follows:\n",
    "\n",
    "    for every iteration of training:\n",
    "        sample a mini-batch B of training data;\n",
    "        generate adversarial examples for B using the current model parameters;\n",
    "        train the model on a mixture of clean and adversarial samples;\n",
    "\n",
    "This algorithm is known as **adversarial training**. Essentially, rather than augmenting the data with adversarial examples for a *fixed* model, we allow the adversarials to adapt to the model as it is trained. The resulting models exhibit much greater robustness to a broader variety of attacks than models that are trained on a static data set. Mathematically, this procedure is an approximation to solving the optimization problem\n",
    "$$\n",
    "    \\min_\\theta\\mathbb{E}\\left[\\max_\\delta L(X + \\delta, Y, \\theta)\\right].\n",
    "$$\n",
    "This formulation was first articulated by [Madry et al. (2017)](https://arxiv.org/pdf/1706.06083.pdf), who introduced adversarial training with the projected gradient descent (PGD) attack. Back in 2017, this was the best known defense. At the time of this writing, PGD-based adversarial training is still considered a strong defense and in fact forms the foundation of many contemporary defenses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52952e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "    \n",
    "    x_train = x_train.astype(np.float32) / 255\n",
    "    x_test = x_test.astype(np.float32) / 255\n",
    "    \n",
    "    y_train = tf.keras.utils.to_categorical(y_train)\n",
    "    y_test = tf.keras.utils.to_categorical(y_test)\n",
    "    \n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1)\n",
    "    \n",
    "    return (x_train, y_train), (x_test, y_test), (x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96daaae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test), (x_val, y_val) = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fb7de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    input_layer = tf.keras.layers.Input(shape=(32, 32, 3))\n",
    "    output = tf.keras.applications.ResNet50(\n",
    "        include_top=False,\n",
    "        weights=None,\n",
    "        input_tensor=None,\n",
    "        input_shape=[32, 32, 3],\n",
    "        pooling='avg',\n",
    "        classes=10\n",
    "    )(input_layer)\n",
    "    output = tf.keras.layers.LayerNormalization()(output)\n",
    "    output = tf.keras.layers.Dense(10)(output)\n",
    "    \n",
    "    model = tf.keras.Model(input_layer, output)\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[tf.keras.metrics.CategoricalAccuracy()],\n",
    "        optimizer=tf.keras.optimizers.Adam()\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e1df53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fb_attack(model, attack, x_data, y_data, epsilon=.03, batch_size=128):\n",
    "    fmodel = fb.models.TensorFlowModel(model, bounds=(0, 1))\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x_data, y_data.argmax(axis=1))).batch(batch_size)\n",
    "    successes = []\n",
    "    x_tilde = np.zeros_like(x_data)\n",
    "    for b, (x_batch, y_batch) in enumerate(tqdm(dataset)):\n",
    "        _, clipped_advs, _ = attack(fmodel, x_batch, y_batch, epsilons=epsilon)\n",
    "        x_tilde[b*batch_size:b*batch_size + clipped_advs.shape[0], ...] = clipped_advs\n",
    "    \n",
    "    return x_tilde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf045b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "robust_model = create_model()\n",
    "robust_model.save_weights(CKPT_PATH)\n",
    "best_loss = np.inf\n",
    "\n",
    "epsilon = .03\n",
    "alpha = .8\n",
    "ce_loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "num_batches = int(np.ceil(x_train.shape[0]) / batch_size)\n",
    "for epoch in range(epochs):\n",
    "    for b in trange(num_batches, desc=f'Epoch {epoch+1}/{epochs}'):\n",
    "        start = b * batch_size\n",
    "        end = min((b+1) * batch_size, x_train.shape[0])\n",
    "        x_batch = x_train[start:end]\n",
    "        y_batch = y_train[start:end]\n",
    "        \n",
    "        x_var = tf.Variable(x_batch)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = robust_model(x_var)\n",
    "            loss1 = ce_loss(y_batch, y_pred)\n",
    "        \n",
    "        x_grad = tape.gradient(loss1, x_var)\n",
    "        delta = epsilon * tf.math.sign(x_grad)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = robust_model(x_batch)\n",
    "            y_tilde = robust_model(tf.clip_by_value(x_batch + delta, 0, 1))\n",
    "            loss1 = ce_loss(y_batch, y_pred)\n",
    "            loss2 = ce_loss(y_batch, y_tilde)\n",
    "            \n",
    "            loss = alpha * loss1 + (1 - alpha) * loss2\n",
    "        \n",
    "        grads = tape.gradient(loss, robust_model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, robust_model.trainable_weights))\n",
    "    \n",
    "    _, acc = robust_model.evaluate(x_val, y_val, batch_size=128, verbose=0)\n",
    "    print(f'Validation standard accuracy: {acc}')\n",
    "    x_tilde = fb_attack(robust_model, fb.attacks.FGSM(), x_val, y_val)\n",
    "    loss, acc = robust_model.evaluate(x_tilde, y_val, batch_size=128, verbose=0)\n",
    "    print(f'Validation robust accuracy  : {acc}')\n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "        robust_model.save_weights(CKPT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362f5263",
   "metadata": {},
   "source": [
    "## Fast adversarial training\n",
    "\n",
    "Adversarial training can be highly effective when done right. However, as you probably noticed when running the above code, it can also be extremely slow. This naturally has led to several lines of research into speeding up this training procedure. Perhaps the most remarkable (and aptly-named) is \"fast adversarial training\" proposed by [Wong et al. (2020)](https://arxiv.org/pdf/2001.03994.pdf). The strategy is extremely efficient and peculiar because *it's literally just fast gradient sign*, the very method we dismissed as ineffective earlier!\n",
    "\n",
    "To elaborate, Wong et al. propose a slight variation of FGSM for use in adversarial training that does not exhibit the catastrophic overfitting and label leaking commonly observed with the original version. Specifically, they make two modifications:\n",
    "\n",
    "1. The method does not start at the original inputs, but applies some random noise to them first.\n",
    "2. The step size is modified: Wong et al. recommend using $1.25\\varepsilon$ as the step size instead of the usual $\\varepsilon$.\n",
    "\n",
    "With these two simple modifications, the method not only yields significant *real* increases in robustness, it also remains just as fast as the original FGSM. Let's implement this method below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3a24dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm_fixed(model, x_batch, y_batch, epsilon=.03):\n",
    "    alpha = 1.25 * epsilon\n",
    "    deltas = tf.Variable(np.random.uniform(-epsilon, epsilon, size=x_batch.shape))\n",
    "    \n",
    "    ce_loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(x_batch + deltas)\n",
    "        loss = ce_loss(y_batch, y_pred)\n",
    "    \n",
    "    grad = tape.gradient(loss, deltas)\n",
    "    deltas.assign_add(alpha * tf.sign(grad))\n",
    "    deltas.assign(tf.clip_by_value(deltas, -epsilon, epsilon))\n",
    "    \n",
    "    return np.clip(x_batch + deltas.numpy(), 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17dec89",
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_model = create_model()\n",
    "fast_model.save_weights(CKPT_PATH)\n",
    "\n",
    "epsilon = .03\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "num_steps = 10\n",
    "num_batches = int(np.ceil(x_train.shape[0]) / batch_size)\n",
    "best_loss = np.inf\n",
    "for epoch in range(epochs):\n",
    "    for b in trange(num_batches, desc=f'Epoch {epoch+1}/{epochs}'):\n",
    "        start = b * batch_size\n",
    "        end = min((b+1) * batch_size, x_train.shape[0])\n",
    "        \n",
    "        x_batch = x_train[start:end]\n",
    "        y_batch = y_train[start:end]\n",
    "        \n",
    "        x_adv_batch = fgsm_fixed(fast_model, x_batch, y_batch, epsilon)\n",
    "        \n",
    "        fast_model.train_on_batch(x_adv_batch, y_batch)\n",
    "    \n",
    "    _, clean_acc = fast_model.evaluate(x_val, y_val, batch_size=batch_size, verbose=0)\n",
    "    \n",
    "    x_advs = fb_attack(fast_model, fb.attacks.LinfPGD(steps=num_steps), x_val, y_val)\n",
    "    loss, rob_acc = fast_model.evaluate(x_advs, y_val, batch_size=batch_size, verbose=0)\n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "        fast_model.save_weights(CKPT_PATH)\n",
    "    \n",
    "    print(f'Clean validation accuracy : {100*clean_acc:.2f}%')\n",
    "    print(f'Robust validation accuracy: {100*rob_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed31777",
   "metadata": {},
   "source": [
    "## A certified defense\n",
    "\n",
    "One major issue with adversarial training is that it only offers *empirical* robustness. In general, there is no way to know whether an adversarially trained model will be robust to *unseen* or *adaptive* attacks. Indeed, many defenses have been broken precisely because they failed to account for new attacks and only optimized robustness on existing ones. In some sense, this is inevitable: we can only verify robustness on attacks that we know.\n",
    "\n",
    "That said, it would be useful if we had *provable* or *certified* defenses: defenses which can guarantee mathematically that no adversarial samples exist *at all* within the given threat model. This implies that no adversarial attack, no matter how strong, could possibly find any adversarial examples provided the threat model is respected. Such methods do exist, but as you can probably imagine, certification doesn't come for free: often, certified defenses require considerably more computational resources than uncertified ones, and they tend to be empirically *less* robust than adversarially trained models.\n",
    "\n",
    "One of the most popular certified defense methods is **randomized smoothing** ([Cohen et al. (2019)](https://arxiv.org/pdf/1902.02918.pdf)). It has several attractive properties that other certified defenses tend to lack:\n",
    "\n",
    "* It can be applied to any existing model without having to retrain it.\n",
    "* It operates on a per-sample basis, giving individualized robustness certificates for different samples.\n",
    "* It is easy to implement.\n",
    "* It can be parallellized to some extent.\n",
    "* It has only one parameter that needs to be set in order for it to work.\n",
    "* The robustness certificate is easy to compute.\n",
    "\n",
    "Its basic operation is illustrated below.\n",
    "\n",
    "<figure>\n",
    "    <img src=\"images/rs.png\">\n",
    "    <figcaption>Illustration of the basic operation of randomized smoothing. Figure due to Cohen et al.</figcaption>\n",
    "</figure>\n",
    "\n",
    "Mathematically, randomized smoothing converts a given classifier $f$ into a \"smoothed model\" $g$ using the formula\n",
    "$$\n",
    "    g(x) = \\underset{y}{\\mathrm{argmax}} \\Pr[f(x + \\delta) = y]\n",
    "$$\n",
    "where $\\delta \\sim \\mathcal{N}(0, \\sigma^2I)$. Essentially, we compute $g(x)$ by corrupting the input $x$ with random perturbations $\\delta$ sampled from an isotropic zero-mean Gaussian distribution $\\mathcal{N}(0, \\sigma^2I)$. The variance $\\sigma^2$ is the only parameter required for randomized smoothing to work. Larger values yield more robust classifiers with larger certified radii around the individual samples, but this naturally comes at a greater cost to accuracy. The certified robustness radius for a given sample is then determined by the simple formula\n",
    "$$\n",
    "    \\rho(x) = \\frac{\\sigma}{2}\\left( \\Phi^{-1}(p_A) - \\Phi^{-1}(p_B) \\right)\n",
    "$$\n",
    "where $\\Phi^{-1}$ is the inverse standard Gaussian CDF. Remark that:\n",
    "\n",
    "* The certified radius is *linear* in the standard deviation $\\sigma$.\n",
    "* The radius increases with the margin between the top-2 predicted class probabilities $p_A$ and $p_B$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6beb1faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from art.estimators.certification.randomized_smoothing import PyTorchRandomizedSmoothing\n",
    "rs_model = PyTorchRandomizedSmoothing(standard_model,\n",
    "                                      loss=torch.nn.NLLLoss(),\n",
    "                                      input_shape=(3, 32, 32),\n",
    "                                      nb_classes=10,\n",
    "                                      clip_values=(0, 1),\n",
    "                                      scale=.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bdafb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test = rb.data.load_cifar10(n_examples=128)\n",
    "y_pred = rs_model.predict(x_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22b2fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = (y_pred.argmax(axis=1) == y_test.numpy()).mean()\n",
    "print(f'Accuracy: {acc:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39c77e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(0, x_test.shape[0])\n",
    "p, r = rs_model.certify(x_test[:idx+1], n=100)\n",
    "y = y_test[idx]\n",
    "\n",
    "plt.imshow(np.transpose(x_test[idx], [1, 2, 0]))\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(f'True class     : {to_name(y)}')\n",
    "print(f'Predicted class: {to_name(p[0])}')\n",
    "print(f'Radius         : {r[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55569043",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05a1709",
   "metadata": {},
   "source": [
    "## Gradient masking\n",
    "\n",
    "Especially in the early days of adversarial robustness research, many defenses were proposed that essentially boiled down to applying JPEG compression or similar pre-processing techniques to improve robustness. The core idea was that adversarial perturbations were likely very fragile themselves, and so small corruptions in the input such as random noise or compression artifacts could probably neutralize them without affecting performance too much.\n",
    "\n",
    "Let's take a moment to explore the effectiveness of JPEG compression as an adversarial defense. This mechanism is built into the ART library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceba6ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from art.defences.preprocessor import JpegCompression\n",
    "compression = JpegCompression(clip_values=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ea4d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test = rb.data.load_cifar10(n_examples=128, data_dir=DATA_DIR)\n",
    "x_jpeg, _ = compression(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a33421",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack = BasicIterativeMethod(\n",
    "    PyTorchClassifier(\n",
    "        model=standard_model,\n",
    "        clip_values=(0, 1),\n",
    "        loss=torch.nn.CrossEntropyLoss(),\n",
    "        input_shape=[3, 32, 32],\n",
    "        nb_classes=10),\n",
    "    eps=.03,\n",
    "    eps_step=.001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09323850",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_advs = attack.generate(x=x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9589d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_advs_jpeg, _ = compression(x_advs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9a93d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = rb.utils.clean_accuracy(standard_model, torch.from_numpy(x_advs_jpeg), y_test, device=DEVICE)\n",
    "print(f'Accuracy: {acc:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca752dd",
   "metadata": {},
   "source": [
    "Adding JPEG compression to adversarially perturbed samples does not seem to help much at all. However, for some reason, many papers (including most notably [Guo et al. (2018)](https://openreview.net/forum?id=SyJ7ClWCb)) proposed it as an effective defense... What's going on here?\n",
    "\n",
    "To understand the issue here, we need to delve into the precise attack methodology used by prior work to assess robustness. Specifically, most works (including Guo et al. (2018)) applied a \"model-agnostic\" attack which did not take into account the details of the original model. Essentially, what they did was this:\n",
    "\n",
    "1. Take an existing accurate model $f$.\n",
    "2. Add JPEG compression (or some other transformation) as a pre-processing step to $f$, obtaining a defended model $h$.\n",
    "3. Attack $h$ directly with existing attacks such as FGSM or BIM.\n",
    "\n",
    "By contrast, we generated adversarial examples for $f$ (the original, undefended model) and fed those to the defended model $h$. This is known as a **transfer attack**, where the adversarial examples are not generated for the victim directly but rather for a different \"surrogate\" model. These attacks are useful when you don't know the precise details of the model you're attacking, but in particular transfer attacks are also really good at beating JPEG compression and other pre-processing based defenses.\n",
    "\n",
    "This method of using transfer attacks to bypass defenses like JPEG compression was famously introduced by [Athalye et al. (2018)](http://proceedings.mlr.press/v80/athalye18a/athalye18a.pdf). They attributed the failure of direct attacks to a phenomenon called **gradient masking**. As the name implies, gradient masking occurs when a model (intentionally or not) distorts its own gradient information. This can happen when the model incorporates non-differentiable or numerically unstable operations, such as JPEG compression. Such operations thwart adversarial attacks that crucially rely on this information, such as fast gradient sign and the basic iterative method. We can visualize this phenomenon using the loss surface of the model. A model that does not mask gradients will have a loss surface that looks like this:\n",
    "\n",
    "<figure>\n",
    "    <img src=\"images/loss1.png\" width=\"400px\">\n",
    "    <figcaption>Typical loss surface of a standard classifier. Different colors correspond to different predicted classes, and the height of the terrain corresponds to the classifier's confidence in the correct label. Image due to <a href=\"https://nicholas.carlini.com/slides/2021_sspr_stillnotrobust.pdf\">Nicholas Carlini</a>.</figcaption>\n",
    "</figure>\n",
    "\n",
    "The surface is very smooth, and simply following the direction of the gradient will eventually allow you to reach a different class, thus possibly resulting in an adversarial example. When a model is masking gradients, however, we get this picture:\n",
    "\n",
    "<figure>\n",
    "    <img src=\"images/loss2.png\" width=\"400px\">\n",
    "    <figcaption>Loss surface of a gradient masking classifier. Image due to <a href=\"https://nicholas.carlini.com/slides/2021_sspr_stillnotrobust.pdf\">Nicholas Carlini</a>.</figcaption>\n",
    "</figure>\n",
    "\n",
    "The loss surface is essentially unchanged, but it has become much more irregular, with many additional valleys and peaks that complicate the landscape. If you simply follow the direction of the gradient to find an adversarial, you will likely get stuck in a local optimum with no obvious way to progress. This situation can be easily remedied, however, via all sorts of simple tricks:\n",
    "\n",
    "* Approximating the gradient using numerical techniques such as finite differences instead of automatic differentiation.\n",
    "* Randomly sampling around the input to smooth out the loss surface.\n",
    "* Using more sophisticated optimization methods that incorporate momentum.\n",
    "* Replacing non-differentiable components of the model by differentiable approximations.\n",
    "\n",
    "Famously, by applying these simple tricks, Athalye et al. completely broke 6 out of 9 defenses submitted to ICLR 2018 before the conference even took place!\n",
    "\n",
    "It is important to appreciate the tempting and highly intuitive, yet ultimately fallacious, reasoning that underpinned the early \"successful\" defenses against adversarial examples. Goodfellow et al. based their FGSM defense on the **linearity hypothesis**, the idea that adversarial examples are caused by excessive linearity of the models in the vicinity of natural data points. Hence, the accepted wisdom at the time was that we merely needed to penalize linear behavior in order to get rid of adversarials. Most of the published defenses between 2014 and 2018 focussed on this idea and simply proposed different ways to efficiently penalize linear behavior. JPEG compression is one example of this: because the compression step is not differentiable, gradient-based attacks applied to the defended model will fail since there's no useful gradient information left after compression. As we have seen, however, this doesn't actually make the model robust: it can still be easily fooled if we just ignore the compression layer.\n",
    "\n",
    "In a sense, the adversarial machine learning community was \"overfitting\" on the FGSM and similar attacks that fundamentally operated on simple transformations of the loss gradient. More sophisticated attacks, such as those proposed by [Carlini & Wagner (2016)](https://arxiv.org/pdf/1608.04644.pdf) or Athalye et al., could easily evade such defenses.\n",
    "\n",
    "Thanks to the efforts of Athalye et al., the adversarial ML community learned an important lesson:\n",
    "\n",
    ">Always evaluate against *adaptive* adversaries.\n",
    "\n",
    "When testing the effectiveness of a defense, it is not enough to show that the defense works against *known, existing* attacks; one must also take into account new attacks that are specifically designed to circumvent the defense you're using. This can require considerable creativity, but it is absolutely necessary to ensure that the defense has any merit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502e410a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8629e8e0",
   "metadata": {},
   "source": [
    "## No free lunch\n",
    "\n",
    "A natural question to ask is whether robustness to *adversarial* perturbations (which are, in some sense, \"worst-case\") also provides robustness to *common* perturbations such as random noise, blurring, fog and other physical distortions. To evaluate the robustness of classifiers to such common corruptions, the CIFAR-10-C (\"corrupted\") was proposed by [Hendrycks & Dietterich (2019)](https://openreview.net/forum?id=HJz6tiCqYm). As the name implies, this data set is comprised of samples from CIFAR-10 which have been subjected to all sorts of realistic corruptions models might actually face in reality.\n",
    "\n",
    "The CIFAR-10-C data set is also included in RobustBench, so we can easily evaluate against it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde13a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "corruptions = ['fog']\n",
    "x_test_c, y_test_c = rb.data.load_cifar10c(n_examples=128, corruptions=corruptions, severity=5)\n",
    "x_test, y_test = rb.data.load_cifar10(n_examples=128)\n",
    "\n",
    "for model_name in ['Standard', 'Engstrom2019Robustness', 'Rice2020Overfitting', 'Carmon2019Unlabeled']:\n",
    "    model = rb.load_model(model_name, dataset='cifar10', threat_model='Linf')\n",
    "    c_acc = rb.utils.clean_accuracy(model, x_test_c, y_test_c)\n",
    "    acc = rb.utils.clean_accuracy(model, x_test, y_test)\n",
    "    print(f'Model: {model_name}')\n",
    "    print(f'    CIFAR-10 accuracy  : {acc:.2%}')\n",
    "    print(f'    CIFAR-10-C accuracy: {c_acc:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e03a663",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b177bee",
   "metadata": {},
   "source": [
    "## Practical guidelines and key take-aways\n",
    "\n",
    "First and foremost: **always consider whether adversarial robustness is really worth it, because it might not be**. Increasing robustness generally comes at the cost of lower accuracy and higher computational overhead both during training as well as at inference time. Moreover, increasing *adversarial* robustness often also entails lowering robustness to other, more realistic corruptions such as random noise. Consider these questions:\n",
    "\n",
    "* Is there a realistic scenario where an adversary might use adversarial examples to manipulate the system? Often (but certainly not always!), if an attacker has the level of access required to introduce adversarial examples into your system, would it not be easier for them to just install some ransomware instead?\n",
    "* If there is a realistic scenario, are there other ways of dealing with this problem? In self-driving cars, for example, adversarial attacks can be thwarted by using other sources of information such as GPS and LIDAR. Mission-critical systems should in general have multiple layers of security and redundancies. Machine learning models already have significant probabilities of error in the benign setting, so even in the absence of adversaries you should not exclusively rely on a single model.\n",
    "* If adversarial examples cause you to question the general reliability of the model, consider resorting to explainable AI techniques, more diverse data sets and more thorough evaluations before deployment. Maybe look at methods which improve model calibration, such as [MixUp](https://keras.io/examples/vision/mixup/) and [Dirichlet calibration](https://github.com/dirichletcal/dirichletcal.github.io).\n",
    "\n",
    "If you *do* decide it is worth it, the following guidelines can be useful:\n",
    "\n",
    "* **Avoid creating your own defense.** The vast majority of reasonable ideas for adversarial defense have already been tried and have been shown to fail. Getting these defenses right is extremely hard, so it is best to stick with tried-and-true methods that have been publicly verified by many researchers over many years. A similar guideline is widely accepted within the field of cryptography, where it has come to be known as [Schneier's Law](https://www.schneier.com/blog/archives/2011/04/schneiers_law.html):\n",
    "\n",
    "> Anyone can invent a security system that they themselves cannot break. When someone hands you a security system and says, “I believe this is secure,” the first thing you have to ask is, “Who the hell are you?” Show me what you’ve broken to demonstrate that your assertion of the system’s security means something.\n",
    "\n",
    "A reliable source for effective defenses is the [RobustBench leaderboard](https://robustbench.github.io/#leaderboard).\n",
    "\n",
    "* Decide on a **threat model**. Typical threat models include $L_p$ norms with $p \\in \\{ 2, \\infty \\}$ and patch attacks. Patch attacks are considered to be more realistic, and specialized defenses against these attacks are often better than defenses against $L_p$ attacks ([Levine & Feizei (2020)](https://proceedings.neurips.cc/paper/2020/file/47ce0875420b2dbacfc5535f94e68433-Paper.pdf)). A reasonable estimate of the adversary's \"budget\" (i.e., the size of the $L_p$ perturbation, maximum dimensions of the patch, etc) is highly specific to the problem, however. Keep in mind that defending against a stronger adversary will likely mean creating a more complicated model with lower accuracy, so you will have to strike a balance.\n",
    "* **Adversarial training** remains the most efficient and effective general-purpose defense method. Most new defenses incorporate adversarial training in some way, so use this as your starting point. [The adversarial robustness toolbox](https://adversarial-robustness-toolbox.readthedocs.io/en/latest/modules/defences/trainer.html) provides a flexible interface for applying adversarial training to any model.\n",
    "* Always use **early stopping** on the robust accuracy of a held-out validation set with adversarial training, otherwise overfitting is almost guaranteed to occur. [Rice et al. (2020)](http://proceedings.mlr.press/v119/rice20a/rice20a.pdf)\n",
    "* Combine adversarial training with [model weight averaging](https://www.tensorflow.org/addons/api_docs/python/tfa/optimizers/MovingAverage) and data augmentation techniques such as [CutMix](https://keras.io/examples/vision/cutmix/), [MixUp](https://keras.io/examples/vision/mixup/) and [AugMix](https://github.com/google-research/augmix). [Rebuffi et al. (2021)](https://openreview.net/pdf?id=kgVJBBThdSZ)\n",
    "* Incorporating **artificial data** created by generative models also helps. [Gowal et al. (2021)](https://openreview.net/pdf?id=0NXUSlb6oEu)\n",
    "* **Unlabeled data** can also be leveraged using the simple scheme of *self-training* to improve robustness. [Carmon et al. (2019)](https://github.com/yaircarmon/semisup-adv)\n",
    "* Use the **[AutoAttack library](https://github.com/fra31/auto-attack)** (included with [RobustBench](https://robustbench.github.io)) to evaluate robustness. Do not forget to run AutoAttack with `version='rand'`, otherwise the result may be misleading in case of randomized defenses and gradient masking.\n",
    "\n",
    "Common pitfalls to watch out for when performing a robustness evaluation:\n",
    "\n",
    "* Iterative attacks should perform better than single-step attacks.\n",
    "* Increasing the attack budget should strictly increase success rate. In particular, for sufficiently strong attacks (such as $L_p$ perturbations with unbounded norm), the success rate should be 100%.\n",
    "* Random sampling should not perform better than adversarial attacks.\n",
    "\n",
    "If any of the above criteria seem to be violated, this is a strong indication that the robustness you measured on your model is misleading. This could be due to e.g. gradient masking or incorrectly implemented adversarial attacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df9c54c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
