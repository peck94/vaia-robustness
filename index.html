<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Adversarial attacks and defenses</title>

		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="dist/theme/white.css" id="theme">

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css" id="highlight-theme">
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<section>
					<h3>Adversarial machine learning</h3>
				</section>

				<section>
					<h4>Who am I</h4>

					<div style="width: 100%">
						<div style="float: left; width: 50%">
							<p><img src="img/pfp.jpg"></p>
							<p style="font-family: monospace; font-size: 15pt">jonathan.peck@ugent.be</p>
						</div>

						<div style="float: right; width: 50%">
							<p>Jonathan Peck</p>
							<p class="fragment">Post-doc @ UGent</p>
							<p class="fragment">TWIST @ De Sterre</p>
							<p class="fragment">DAMBI @ IRC</p>
						</div>
					</div>
				</section>

				<section>
					<h4>Standard ML</h4>

					<p>Samples are <i>independent</i> and <i>identically distributed</i> (i.i.d.)</p>
					<p class="fragment">
						<img src="img/iid.png">
					</p>
				</section>

				<section>
					<h4>Standard ML</h4>

					<img src="img/hotdog.jpg">
				</section>

				<section>
					<h4>Standard ML</h4>

					<img src="img/vapnik.jpg">
				</section>

				<section>
					<h4>Standard ML</h4>

					<p>Impressive results in the i.i.d. setting since ~2013</p>
					<p class="fragment">
						<img src="img/ilsvrc.png">
					</p>
				</section>

				<section>
					<h4>Standard ML</h4>

					<img src="img/model_comparison.png" width="60%">
				</section>

				<section>
					<h4>Caveat</h4>

					<p>Assumption of i.i.d. data can be violated when the model is deployed!</p>
				</section>

				<section>
					<h4>Unusual samples</h4>

					<p><img src="img/unusual.png"></p>
					<p class="fragment">"basketball"</p>
				</section>

				<section>
					<h4>Representation bias</h4>

					<p>Data sets are often amerocentric and eurocentric</p>
					<p class="fragment"><img src="img/geo.png" height="400"></p>
				</section>

				<section>
					<h4>Representation bias</h4>
					<p><img src="img/bridegroom.png" height="500"></p>
				</section>

				<section>
					<h4>Malicious adversaries</h4>

					<img src="img/attacks.png">
				</section>

				<section>
					<h4>Spam filtering</h4>

					<img src="img/spam.png">
				</section>

				<section>
					<h4>Spam filtering</h4>

					<img src="img/spam2.png">
				</section>

				<section>
					<h4>Other scenarios</h4>

					<p>Malware detection (e.g. malicious PDFs, MalConv)</p>
					<p class="fragment">Biometric security systems</p>
					<p class="fragment">Automatic content filtering (e.g. YouTube)</p>
				</section>

				<section>
					<h3>Adversarial examples</h3>
					<p>Type of evasion attack</p>
					<p class="fragment">What is the smallest modification to a "natural" sample that causes the model to make a mistake?</p>
				</section>

				<section>
					<h4>Adversarial examples</h4>

					<img src="img/ostrich.png">

					<p><a href="https://arxiv.org/abs/1312.6199">Szegedy et al. (2013)</a></p>
				</section>

				<section>
					<h4>Adversarial examples</h4>

					<p>Already well-known in "classical" ML (<a href="https://dl.acm.org/doi/abs/10.1145/1081870.1081950">Lowd & Meek (2005)</a>)</p>

					<p class="fragment">
						Unexpected for DNNs due to the "smoothness prior" (<a href="https://ieeexplore.ieee.org/abstract/document/6472238">Bengio et al. (2013)</a>):
						\[
							x \approx x' \implies f(x) \approx f(x')
						\]
					</p>
				</section>

				<section>
					<h4>Adversarial examples</h4>

					<p><strong>Typical formalization.</strong> An input $\tilde{x}$ is <i>adversarial</i> for a sample $(x, y)$ and model $f$ if</p>
					<ul>
						<li class="fragment">$f(x) = y$;</li>
						<li class="fragment">$f(\tilde{x}) \neq y$;</li>
						<li class="fragment">$d(x, \tilde{x}) \leq \varepsilon$</li>
					</ul>
				</section>

				<section>
					<h4>Adversarial examples</h4>

					<p>
						Practical optimization problem:
						$$
							\min_{\delta}~d(x, x + \delta)
							\mbox{ subject to }
							f(x) \neq f(x + \delta)
						$$
					</p>

					<p class="fragment">Can reduce accuracy of SOTA models to 0% for very small $\delta$!</p>
				</section>

				<section>
					<h3>Adversarial attacks</h3>

					<p>How to create adversarial examples</p>
				</section>

				<section>
					<h4>Threat model</h4>

					<img src="img/threat.png">
				</section>

				<section>
					<h4>Threat model</h4>

					<p>
						$L_p$ threat model:
						$$
							d(x, \tilde{x}) = \|x - \tilde{x}\|_p \leq \varepsilon
						$$
					</p>
				</section>

				<section>
					<h4>Threat model</h4>

					<p>
						Patch attacks:
					</p>

					<img src="img/patch.jpg" width="60%">
				</section>

				<section>
					<h4>Threat model</h4>

					<p>NLP: insertions, deletions and substitutions of tokens</p>

					<img src="img/hotflip1.png">
				</section>

				<section>
					<h4>Threat model</h4>

					<p>
						Taxonomy of attacks:
						<ul>
							<li class="fragment">
								White-box
								<ul>
									<li class="fragment">gradient-based</li>
									<li class="fragment">gradient-free</li>
								</ul>
							</li>
							<li class="fragment">
								Black-box
								<ul>
									<li class="fragment">surrogate-based</li>
									<li class="fragment">score-based</li>
									<li class="fragment">decision-based</li>
								</ul>
							</li>
						</ul>
					</p>
				</section>

				<section>
					<h4>L-BFGS attack</h4>

					<p>Minimize $c\|\delta\| + L(x + \delta, y, f)$</p>

					<p class="fragment">Straightforward but inefficient</p>
				</section>
				
				<section>
					<h4>L-BFGS attack</h4>
					<img src="img/ostrich.png">
					<p><a href="https://arxiv.org/abs/1312.6199">Szegedy et al. (2013)</a></p>
				</section>

				<section>
					<h4>Fast gradient sign attack</h4>

					<p>We wish to maximize $L(x + \delta, y, f)$.</p>

					<p class="fragment">
						<strong>Idea.</strong> First-order Taylor expansion:
						$$
							L(x + \delta, y, f) \approx L(x, y, f) + \langle \nabla_xL(x, y, f), \delta \rangle.
						$$
					</p>

					<p class="fragment">
						Let $\delta = \varepsilon\mathrm{sign}(\nabla_xL(x, y, f))$. Then $\|\delta\|_\infty = \varepsilon$ and
						$$
							L(x + \delta, y, f) \approx L(x, y, f) + \varepsilon\|\nabla_xL(x, y, f)\|_1.
						$$
					</p>
				</section>

				<section>
					<h4>Fast gradient sign attack</h4>

					<p>
						$$
							\tilde{x} = x + \varepsilon\mathrm{sign}(\nabla_xL(x, y, f)).
						$$
					</p>

					<p class="fragment">Very fast</p>
					<p class="fragment">Requires access to the gradients of the loss</p>
				</section>

				<section>
					<h4>Fast gradient sign attack</h4>
					<img src="img/fgs.png">
					<p><a href="https://arxiv.org/abs/1412.6572">Goodfellow et al. (2014)</a></p>
				</section>

				<section>
					<h4>PGD attack</h4>

					<p>
						Let $\tilde{x}_0 \gets x$ and iterate:
						$$
							\tilde{x}_{t+1} \gets \Pi_S(\tilde{x}_t + \varepsilon\mathrm{sign}(\nabla_xL(x, y, f))
						$$
					</p>

					<p class="fragment">Essentially multi-step FGSM with projection</p>
					<p class="fragment">Very strong, moderately fast</p>
				</section>

				<section>
					<h4>Notebook demo</h4>
				</section>

				<section>
					<h4>Surrogate black-box attacks</h4>
					<p><img src="img/papernot.png"></p>
					<p><a href="https://arxiv.org/abs/1602.02697">Papernot et al. (2016)</a></p>
				</section>

				<section>
					<h4>One-pixel attack</h4>
					<img src="img/one_pixel.png">
				</section>

				<section>
					<h4>Image transformations</h4>
					<img src="img/rotate.png">
				</section>

				<section>
					<h4>Universal adversarial perturbations</h4>
					<img src="img/uap.png" height="400px">

					<p><a href="https://arxiv.org/abs/1610.08401">Moosavi-Dezfooli et al. (2016)</a></p>
				</section>

				<section>
					<h4>Adversarial examples IRL</h4>
					<p><img src="img/stop.png"></p>
					<p class="fragment">"speed limit" instead of "stop sign"</p>
				</section>

				<section>
					<h4>Adversarial examples IRL</h4>
					<iframe width="560" height="315" src="https://www.youtube.com/embed/i1sp4X57TL4" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
				</section>

				<section>
					<h4>Adversarial examples IRL</h4>
					<iframe width="560" height="315" src="https://www.youtube.com/embed/YXy6oX1iNoA" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
				</section>

				<section>
					<h4>Adversarial examples IRL</h4>

					<img src="img/mask.png" width="50%">
				</section>

				<section>
					<h4>Adversarial examples for NLP</h4>

					<img src="img/hotflip1.png">
				</section>

				<section>
					<h4>Adversarial examples for NLP</h4>

					<img src="img/hotflip2.png">
				</section>

				<section>
					<h4>Universal trigger phrases</h4>
					<img src="img/triggers.png">

					<p><a href="https://aclanthology.org/D19-1221.pdf">Wallace et al. (2019)</a></p>
				</section>

				<section>
					<h4>ChatGPT version 2023-01-09</h4>

					<table>
						<tr>
							<td><input type="text" value="Prove that the square of 2 is irrational." style="width: 100%" readonly></td>
						</tr>
						<tr>
							<td>
								<textarea rows="20" cols="100" readonly>The proof that the square of 2 is irrational is a well-known proof, which can be shown by contradiction. If we assume that the square of 2 is rational, then it can be written in the form of a fraction, where the numerator and denominator are integers. Let's represent the square of 2 as a fraction, where a and b are integers and b is not equal to zero.
	(2^2) = (a/b)
Expanding the left side of the equation:
	4 = (a/b)
We can then multiply both sides of the equation by b:
	4b = a
Since a and b are integers, a must also be divisible by 4. So let's write a as 4x for some integer x:
	4b = 4x
Dividing both sides by 4:
	b = x
So, we have a = 4x and b = x, which means that a/b = 4, and thus the square of 2 can be expressed as a ratio of integers. But this contradicts our assumption that the square of 2 is irrational. Therefore, the square of 2 is irrational.</textarea>
							</td>
						</tr>
					</table>
				</section>

				<section>
					<h4>Resume screening</h4>

					<img src="img/vdab_orig.png">
				</section>

				<section>
					<h4>Resume screening</h4>

					<img src="img/vdab_adv.png">
				</section>

				<section>
					<h4>Adversarial examples for audio</h4>

					<img src="img/audio.png">
				</section>

				<section>
					<h4>Adversarial examples for audio</h4>

					<a href="https://cseweb.ucsd.edu//~yaq007/imperceptible-robust-adv.html" target="_blank">Examples</a>
				</section>

				<section>
					<h4>Misleading interpretability techniques</h4>

					<img src="img/interpret.png">
				</section>

				<section>
					<h4>Beyond simple perturbations</h4>

					<img src="img/ssim.png">
				</section>

				<section>
					<h4>Beyond simple perturbations</h4>

					<p>Simple additive norm-bounded perturbations are a toy problem!</p>
					<p class="fragment">Some work has used generative models for more sophisticated attacks, such as <a href="https://arxiv.org/pdf/2006.16241.pdf">DeepAugment</a></p>
				</section>

				<section>
					<h4>Beyond simple perturbations</h4>

					<img src="img/deepaugment.png">
				</section>

				<section>
					<h3>Adversarial defenses</h3>
					
					<p>How to cope with adversarial examples</p>
				</section>

				<section>
					<h4>Kerckhoffs's principle</h4>

					<blockquote>
						A cryptosystem should be secure even if everything about the system, except the key, is public knowledge.
					</blockquote>
				</section>

				<section>
					<h4>Adversarial defenses</h4>

					<ul>
						<li><strong>Denoising.</strong> Remove the adversarial noise from the samples.</li>
						<li class="fragment"><strong>Detection.</strong> Detect the presence of adversarial noise.</li>
						<li class="fragment"><strong>Hardening.</strong> Make the model immune to adversarial noise.</li>
					</ul>
				</section>

				<section>
					<h4>Adversarial defenses</h4>

					<ul>
						<li><strong>Certified.</strong> Mathematical proof that no $\delta$ with $\|\delta\| \leq \varepsilon$ will fool the model.</li>
						<li class="fragment"><strong>Uncertified.</strong> No such guarantees; only empirical evidence.</li>
					</ul>

					<p class="fragment">Certified defenses are preferred, but more difficult to handle</p>
				</section>

				<section>
					<h4>Adversarial training</h4>
					<p>Just train on the adversarial examples as well</p>
					<p class="fragment">Uncertified but simple</p>
					<p class="fragment">Effectiveness heavily depends on attack</p>
				</section>

				<section>
					<h4>Robust optimization</h4>

					<p>
						Standard ML:
						$$
							\min_\theta~\mathbb{E}\left[ L(X, Y, \theta) \right].
						$$
					</p>

					<p class="fragment">
						Robust ML:
						$$
							\min_\theta~\mathbb{E}\left[ \max_\Delta L(X + \Delta, Y, \theta) \right].
						$$
					</p>
				</section>

				<section>
					<h4>Robust optimization</h4>

					<p>
						<strong>Practical implementation.</strong> For each minibatch,
						<ol>
							<li class="fragment"><strong>Inner maximization.</strong> Use adversarial attacks to construct adversarial training samples.</li>
							<li class="fragment"><strong>Outer minimization.</strong> Update model parameters using these worst-case samples.</li>
						</ol>
					</p>

					<p class="fragment">Typically uses PGD</p>

					<p class="fragment">
						Can be certified sometimes
						(<a href="https://arxiv.org/abs/1710.10571">Sinha et al. (2017)</a>,
						<a href="http://proceedings.mlr.press/v97/zhang19p.html">Zhang et al. (2019)</a>)
					</p>
				</section>

				<section>
					<h4>Robust overfitting</h4>

					<img src="img/robust_overfit.png" width="60%">

					<p><a href="http://proceedings.mlr.press/v119/rice20a/rice20a.pdf">Rice et al. (2020)</a></p>
				</section>

				<section>
					<h4>Fast adversarial training</h4>

					<p>PGD-based adversarial training is extremely slow</p>
					<p class="fragment">FGS-based adversarial training doesn't work (<a href="https://arxiv.org/pdf/1611.01236.pdf">Kurakin et al. 2016</a>)</p>
					<p class="fragment">... or does it?</p>
				</section>

				<section>
					<h4>Fast adversarial training</h4>

					<p>FGS-based adversarial training can work if implemented carefully (<a href="https://arxiv.org/pdf/2001.03994.pdf">Wong et al. 2020</a>)</p>
				</section>

				<section>
					<h4>Fast adversarial training</h4>

					<p>
						Two modifications of FGS:
						<ol>
							<li class="fragment">apply random noise to the initial input</li>
							<li class="fragment">use $1.25\varepsilon$ as the step size instead of $\varepsilon$</li>
						</ol>
					</p>

					<p class="fragment">Results are competitive with PGD but just as fast as regular training!</p>
				</section>

				<section>
					<h4>Randomized smoothing</h4>

					$$\begin{aligned}
						g(x) &= \mathrm{argmax}_y~\Pr[f(x + \eta) = y]\\
						&\mbox{ where }\eta \sim \mathcal{N}(0, \sigma^2I).
					\end{aligned}$$

					<p class="fragment">
						Certified robustness radius:
						$$
							R = \frac{\sigma}{2}\left( \Phi^{-1}(p_A) - \Phi^{-1}(p_B) \right).
						$$
					</p>
				</section>

				<section>
					<h4>Randomized smoothing</h4>

					<img src="img/rs.png">
				</section>

				<section>
					<h4>Notebook demo</h4>
				</section>

				<section>
					<h4>Denoised smoothing</h4>

					<p>Randomized smoothing lowers accuracy because of noise</p>

					<p class="fragment">Simply add a denoiser!</p>

					<p class="fragment">
						$$\begin{aligned}
							g(x) &= \mathrm{argmax}_y~\Pr[f(D(x + \eta)) = y]\\
							&\mbox{ where }\eta \sim \mathcal{N}(0, \sigma^2I).
						\end{aligned}$$

						where $D$ is a denoising network
					</p>
				</section>

				<section>
					<h4>De-randomized smoothing</h4>

					<img src="img/derandomized_smoothing.png">

					<p><a href="https://proceedings.neurips.cc/paper/2020/file/47ce0875420b2dbacfc5535f94e68433-Paper.pdf">Levine &amp; Feizi (2020)</a></p>
				</section>

				<section>
					<h4>Combination therapies</h4>

					<p>Modern defenses usually combine adversarial training with other tricks</p>
				</section>

				<section>
					<h4>Combination therapies</h4>

					<p>
						<a href="https://arxiv.org/pdf/2103.01946.pdf">Rebuffi et al. (2021)</a>:
						<ul>
							<li class="fragment">adversarial training</li>
							<li class="fragment">model weight averaging</li>
							<li class="fragment">CutMix</li>
							<li class="fragment">artificial data from generative models (unsupervised!)</li>
						</ul>
					</p>

					<p class="fragment">See also <a href="https://arxiv.org/abs/2302.04638">Wang et al. (2023)</a></p>
				</section>

				<section>
					<h4>SOTA summary</h4>

					<p>
						<table>
							<tr>
								<th>Data</th>
								<th>Threat</th>
								<th>Standard</th>
								<th>Robust</th>
								<th>Best</th>
							</tr>
							<tr>
								<td rowspan="2">CIFAR-10</td>
								<td>$L_2 \leq 0.5$</td>
								<td>95.54%</td>
								<td>84.97%</td>
								<td rowspan="2">99.50%</td>
							</tr>
							<tr>
								<td>$L_{\infty} \leq 8/255$</td>
								<td>93.25%</td>
								<td>70.69%</td>
							</tr>
							<tr>
								<td>CIFAR-100</td>
								<td>$L_{\infty} \leq 8/255$</td>
								<td>75.22%</td>
								<td>42.67%</td>
								<td>96.08%</td>
							</tr>
							<tr>
								<td>ImageNet</td>
								<td>$L_{\infty} \leq 4/255$</td>
								<td>73.76%</td>
								<td>47.60%</td>
								<td>91.10%</td>
							</tr>
						</table>
					</p>

					<p><a href="https://github.com/RobustBench/robustbench">RobustBench leaderboard</a></p>
				</section>

				<section>
					<h4>Arms race</h4>

					<img src="img/race.png">
				</section>

				<section>
					<h4>Arms race</h4>

					<p>Researchers have tried <strong>a lot</strong> of approaches for defense</p>

					<p class="fragment">
						Almost every defense proposed thus far has been broken (<a href="https://nicholas.carlini.com/papers/2020_neurips_adaptiveattacks.pdf">Tramer et al. (2020)</a>)
						<ul>
							<li class="fragment">... by more powerful attacks</li>
							<li class="fragment">... due to researchers' own oversights (e.g. <a href="https://arxiv.org/abs/1802.00420">gradient masking</a>)</li>
						</ul>
					</p>
				</section>

				<section>
					<h4>Gradient masking</h4>

					<img src="img/carlini1.png" width="50%">
				</section>

				<section>
					<h4>Gradient masking</h4>

					<img src="img/carlini2.png" width="50%">
				</section>

				<section>
					<h4>Notebook demo</h4>
				</section>

				<section>
					<h4>Schneier's Law</h4>

					<p>
						<blockquote>
							Any person can invent a security system so clever that they themselves can't think of how to break it.
						</blockquote>
					</p>
				</section>

				<section>
					<h3>Theoretical results</h3>

					<p>Fundamental theoretical understanding of robustness</p>
				</section>

				<section>
					<h4>Lipschitz networks</h4>

					<p>
						Lipschitz continuity:
						$$
							d(f(x), f(x^\prime)) \leq Kd(x, x^\prime)
						$$
					</p>

					<p class="fragment">
						Obviously useful for robustness, so much research interest
					</p>

					<p class="fragment">
						Lipschitz constraint is very restrictive, however...
					</p>
				</section>

				<section>
					<h4>Lipschitz VAEs</h4>

					<img src="img/lipvae.png">

					<p><a href="https://arxiv.org/pdf/2102.07559.pdf">Barrett et al. (2021)</a></p>
				</section>

				<section>
					<h4>Lipschitz VAEs</h4>

					<p>
						$$
							\Pr[\|f(x) - f(\tilde{x})\|_2 \leq r] \geq 1 - \Theta\left(\frac{K\varepsilon^2}{r^2}\right).
						$$
					</p>
				</section>

				<section>
					<h4>No free lunch</h4>

					<p>No non-trivial concept class can be robustly learned in the distribution-free setting against an adversary who can perturb a single input bit.</p>

					<p class="fragment">
						<a href="https://proceedings.neurips.cc/paper/2019/hash/8133415ea4647b6345849fb38311cf32-Abstract.html" target="_blank">Gourdeau et al. (2019)</a>
					</p>
				</section>

				<section>
					<h4>Sample complexity</h4>

					<p>Robust learning, when possible, can require exponentially more samples than standard learning</p>

					<p class="fragment">
						<a href="https://arxiv.org/abs/1906.05815">Diochnos et al. (2019)</a>
					</p>
				</section>

				<section>
					<h4>Robustness vs accuracy</h4>

					<p>Robust accuracy can be at odds with standard accuracy</p>
				</section>

				<section>
					<h4>Robustness vs accuracy</h4>

					<p>
						$$
							y = \begin{cases}
								+1 & \text{wp 50%,}\\
								-1 & \text{wp 50%.}
							\end{cases}
						$$
					</p>

					<p class="fragment">
						$$
						x_1 = \begin{cases}
							+y & \text{wp }p,\\
							-y & \text{wp }1-p.
						\end{cases}
						$$
					</p>

					<p class="fragment">$x_2, \dots, x_{d+1} \sim \mathcal{N}(\eta y, 1)$</p>
				</section>

				<section>
					<h4>Robustness vs accuracy</h4>

					<p>Basically, the label $y$ is $+1$ or $-1$ uniformly at random, $x_1$ is moderately correlated with $y$ (via $p$) and $x_2, \dots, x_{d+1}$ are weakly correlated with $y$ (for large $\eta$).</p>
				</section>

				<section>
					<h4>Robustness vs accuracy</h4>

					<p>
						<strong>Standard classification.</strong> Easy:
						$$
							f(x) = \mathrm{sign}(\langle w, x \rangle)
						$$
						where
						$$
							w = \left(0, \frac{1}{d}, \dots, \frac{1}{d} \right).
						$$
					</p>

					<p class="fragment">Arbitrarily accurate for $d \to \infty$</p>
				</section>

				<section>
					<h4>Robustness vs accuracy</h4>

					<p><strong>Theorem (<a href="https://arxiv.org/pdf/1805.12152.pdf">Tsipras et al. (2018)</a>).</strong> Any classifier that attains at least $1-\delta$ standard accuracy has robust accuracy at most $\frac{p}{1-p}\delta$ against an $\ell_\infty$-bounded adversary with $\varepsilon \geq 2\eta$.</p>

					<p class="fragment"><strong>Corollary.</strong> For $p < 1$, 100% standard accuracy implies 0% robust accuracy!</p>
				</section>

				<section>
					<h4>Robustness vs accuracy</h4>

					<p><a href="https://arxiv.org/pdf/2003.02460.pdf">Yang et al. (2020)</a>: classes from real-world data sets are well-separated, so robustness should be achievable</p>
					<p class="fragment">So why haven't we achieved it?</p>
					<p class="fragment">Probably some form of overfitting...</p>
				</section>

				<section>
					<h4>Robustness vs accuracy</h4>

					<img src="img/separation.png">
				</section>

				<section>
					<h4>Robustness vs accuracy</h4>

					<p>Robustness to adversarial perturbations often hurts robustness to other, more natural corruptions</p>
					<p class="fragment">This may be related to the <strong>frequency</strong> of the noise signals (<a href="https://arxiv.org/abs/1906.08988">Yin et al. (2019)</a>)</p>
				</section>

				<section>
					<h4>Notebook demo</h4>
				</section>

				<section>
					<h4>Brittle features</h4>

					<img src="img/brittle.png">
				</section>

				<section>
					<h4>Brittle features</h4>

					<p>Labels of $\mathcal{D}_{NR}$ look wrong</p>
					<p class="fragment">Samples of $\mathcal{D}_R$ look wrong</p>
					<p class="fragment">Training on $\mathcal{D}$ yields the same results as training on $\mathcal{D}_{NR}$!</p>
					<p class="fragment">Training on $\mathcal{D}_R$ yields robust, accurate model!</p>
				</section>

				<section>
					<h4>Brittle features</h4>

					<p><strong>Hypothesis (<a href="https://arxiv.org/pdf/1905.02175.pdf">Ilyas et al. (2019)</a>).</strong> Data sets contain brittle features which generalize to the test set but do not withstand adversarial attacks.</p>
					<p class="fragment">Standard models include these brittle features because they maximize test accuracy at all costs</p>
				</section>

				<section>
					<h4>Invariance vs sensitivity</h4>

					<p>
						<img src="img/jacobsen.png">
					</p>

					<p><a href="https://arxiv.org/abs/1811.00401">Jacobsen et al. (2019)</a></p>
				</section>

				<section>
					<h3>Conclusions</h3>
				</section>

				<section>
					<p>
						Major open problems:
						<ul>
							<li class="fragment">
								Robustness to one threat model (e.g. $L_2$) does not transfer well to others (e.g. $L_{\infty}$)
							</li>

							<li class="fragment">
								Standard accuracy is often significantly lower than non-robust model
							</li>

							<li class="fragment">
								Perturbation budgets are much smaller than we would like
							</li>

							<li class="fragment">
								Difficult to move beyond the toy problem of small additive perturbations
							</li>
						</ul>
					</p>
				</section>

				<section>
					<h3>Don't panic!</h3>

					<p class="fragment"><img src="img/knock.png"></p>
				</section>

				<section>
					<h4>Don't panic!</h4>

					<p><a href="https://arxiv.org/pdf/1807.06732.pdf">Gilmer et al. (2018)</a>: adversarial perturbations are rarely an actual security concern</p>
					<p class="fragment">More of a <strong>generalization</strong> issue than a <strong>security</strong> problem...</p>
				</section>

				<section>
					<h4>Conclusions</h4>

					<p>Adversarial robustness might <i>not</i> be worth it in practice</p>
					<p class="fragment">Make sure there is a realistic threat model (<a href="https://arxiv.org/pdf/1807.06732.pdf">Gilmer et al. 2018</a>)</p>
					<p class="fragment">Can't you compensate for adversarial examples in some other way?</p>
				</section>

				<section>
					<h4>Conclusions</h4>

					<p>Avoid creating your own defense</p>
					<p class="fragment">Stick to tried-and-true published methods</p>
					<p class="fragment">See <a href="https://github.com/RobustBench/robustbench">RobustBench</a></p>
				</section>

				<section>
					<h4>Conclusions</h4>

					<p>Adversarial training is the basic building block of most modern defenses</p>
					<p class="fragment">Often in conjunction with clever data augmentation techniques (<a href="https://openreview.net/pdf?id=kgVJBBThdSZ">Rebuffi et al. 2021</a>)</p>
					<p class="fragment">Don't forget to use early stopping against robust overfitting!</p>
				</section>

				<section>
					<h4>Conclusions</h4>

					<p>Incorporating artificial data created by generative models also helps (<a href="https://openreview.net/pdf?id=0NXUSlb6oEu">Gowal et al. 2021</a>)</p>
					<p class="fragment">Unlabeled data can also be used with self-training (<a href="https://github.com/yaircarmon/semisup-adv">Carmon et al. 2019</a>)</p>
				</section>

				<section>
					<h4>Conclusions</h4>

					<p>Use <a href="https://github.com/fra31/auto-attack">AutoAttack</a> to evaluate robustness</p>
					<p class="fragment">Make sure to use randomized and plus versions</p>
					<p class="fragment">Unfortunately not (yet) possible for patch attacks...</p>
					<p class="fragment">Think about adaptive attacks (<a href="https://proceedings.neurips.cc/paper/2020/file/11f38f8ecd71867b42433548d1078e38-Paper.pdf">Trame&#768;r et al. 2020</a>)</p>
				</section>

				<section>
					<h4>Conclusions</h4>

					<p>Watch out for gradient masking</p>
					<p class="fragment">Iterative attacks $\geq$ single-step attacks</p>
					<p class="fragment">Higher attack budget $\implies$ better success rate</p>
					<p class="fragment">Unbounded attacks should be 100% successful</p>
					<p class="fragment">Random sampling should not perform better than adversarial attacks</p>
				</section>

				<section>
					<h4>Further reading</h4>

					<ul>
						<li><a href="https://adversarial-ml-tutorial.org/">Adversarial ML tutorial</a></li>
						<li><a href="https://nicholas.carlini.com/">Nicholas Carlini's blog</a></li>
						<li><a href="http://madry-lab.ml/">Madry Lab</a></li>
					</ul>
				</section>

				<section>
					<h4>Frameworks</h4>

					<ul>
						<li><a href="https://adversarial-robustness-toolbox.org/">Adversarial robustness toolbox</a></li>
						<li><a href="https://github.com/fra31/auto-attack">AutoAttack</a></li>
						<li><a href="https://foolbox.readthedocs.io/en/stable/">Foolbox</a></li>
						<li><a href="https://github.com/tensorflow/cleverhans">CleverHans</a></li>
						<li><a href="https://github.com/thu-ml/ares">ARES</a></li>
					</ul>
				</section>

				<section>
					<h4>Benchmarks</h4>

					<ul>
						<li><a href="https://robustbench.github.io">RobustBench</a></li>
						<li><a href="http://robust.art/">RobustART</a></li>
						<li><a href="https://ml.cs.tsinghua.edu.cn/ares-bench/#/leaderboard">ARES-Bench</a></li>
					</ul>
				</section>

				<section>
					<h3>Thank you!</h3>
				</section>
			</div>
		</div>

		<script src="dist/reveal.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				hash: true,
				slideNumber: true,
				transition: 'none',

				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes ]
			});
		</script>
		<script src="plugin/math/math.js"></script>
		<script>
		  Reveal.initialize({
			math: {
			  mathjax: 'https://cdn.jsdelivr.net/gh/mathjax/mathjax@2.7.8/MathJax.js',
			  config: 'TeX-AMS_HTML-full',
			  // pass other options into `MathJax.Hub.Config()`
			  TeX: { Macros: { RR: "{\\bf R}" } }
			},
			plugins: [ RevealMath ]
		  });
		</script>
	</body>
</html>
